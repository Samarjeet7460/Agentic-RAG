{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fcedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bad735",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    docs: List[str]\n",
    "    is_sufficient: bool\n",
    "    iterations: int \n",
    "    max_iterations: int \n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ede932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agentic_RAG:\n",
    "    def __init__(self, path:str):\n",
    "        self.model = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\", temperature=0.2)\n",
    "        self.embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.loader = PyPDFLoader(path)\n",
    "        self.splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=80)\n",
    "        self.parser = JsonOutputParser()\n",
    "        self.docs_retriever = None\n",
    "        self.workflow = None\n",
    "\n",
    "    def ingest_documents(self):\n",
    "        document = self.loader.load()\n",
    "        chunks = self.splitter.split_documents(document)\n",
    "        vector_store = FAISS.from_documents(chunks, embedding=self.embedding)\n",
    "        vector_store.save_local('vector_store')\n",
    "        print(f\"Documents loaded successfully!\")\n",
    "\n",
    "    def load_vectorstore(self):\n",
    "        vector_store = FAISS.load_local(\n",
    "            'vector_store',\n",
    "            embeddings=self.embedding,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        self.docs_retriever = vector_store.as_retriever(search_kwargs={'k':4})\n",
    "\n",
    "    def retrieve(self, state: AgentState):\n",
    "        docs = self.docs_retriever.invoke(state['question'])\n",
    "        return {\n",
    "            'docs': [d.page_content for d in docs],\n",
    "            'iterations': state['iterations'] + 1\n",
    "        }\n",
    "    \n",
    "    def check_sufficient(self, state: AgentState):\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "                Question:\n",
    "                {question}\n",
    "\n",
    "                context:\n",
    "                {context}\n",
    "\n",
    "                Is the context sufficient to fully answer the question?\n",
    "\n",
    "                Respond strictly in JSON:\n",
    "                {{\n",
    "                \"sufficient\": true/false,\n",
    "                \"query\": \"refined search query if insufficient\"\n",
    "                }}\n",
    "            \"\"\",\n",
    "            input_variables=['question','context']\n",
    "        )\n",
    "\n",
    "        chain = prompt | self.model | self.parser\n",
    "        response = chain.invoke({'question': state['question'], 'context': \"\\n\".join(state[\"docs\"])})\n",
    "\n",
    "        return {\n",
    "            'is_sufficient': response['sufficient'],\n",
    "            'query': response.get('query',\"\")\n",
    "        } \n",
    "    \n",
    "    def answer(self, state: AgentState):\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are an expert assistant.\n",
    "\n",
    "            Answer the question in a clear, detailed, elaborate, and well-structured manner\n",
    "            using ONLY the information provided in the context below.\n",
    "\n",
    "            Context: {context}\n",
    "            question: {question}\n",
    "            \"\"\",\n",
    "            input_variables=['question','context']\n",
    "        )\n",
    "\n",
    "        chain = prompt | self.model\n",
    "        response = chain.invoke({'question': state['question'], 'context': \"\\n\".join(state[\"docs\"])})\n",
    "        return {'final_answer': response.content}\n",
    "    \n",
    "    def should_continue(self, state: AgentState):\n",
    "        if state['is_sufficient']:\n",
    "            return 'answer'\n",
    "        if state['iterations'] >= state['max_iterations']:\n",
    "            return 'answer'\n",
    "        return 'retrieve'\n",
    "    \n",
    "    def build_graph(self):\n",
    "        graph = StateGraph(AgentState)\n",
    "\n",
    "        graph.add_node('retrieve', self.retrieve)\n",
    "        graph.add_node('check_sufficient', self.check_sufficient)\n",
    "        graph.add_node('answer', self.answer)\n",
    "\n",
    "        graph.add_edge(START, 'retrieve')\n",
    "        graph.add_edge('retrieve', 'check_sufficient')\n",
    "        graph.add_conditional_edges(\n",
    "            'check_sufficient',\n",
    "            self.should_continue,\n",
    "            {\n",
    "                \"retrieve\": \"retrieve\",\n",
    "                \"answer\": \"answer\"\n",
    "            }\n",
    "        )\n",
    "        graph.add_edge('answer', END)\n",
    "\n",
    "        self.workflow = graph.compile()\n",
    "\n",
    "    def output_result(self, question:str):\n",
    "        if not self.workflow:\n",
    "            self.build_graph()\n",
    "\n",
    "        initial_state: AgentState = {\n",
    "            \"question\": question,\n",
    "            \"query\": \"\",\n",
    "            \"docs\": [],\n",
    "            \"is_sufficient\": False,\n",
    "            \"iterations\": 0,\n",
    "            \"max_iterations\": 5,\n",
    "            \"final_answer\": \"\"\n",
    "        }\n",
    "\n",
    "        result = self.workflow.invoke(initial_state)\n",
    "        print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f111b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = Agentic_RAG(path='ai.pdf')\n",
    "rag.ingest_documents()\n",
    "rag.load_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.output_result(question=\"what is AI and name 5 fields where it is used\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
